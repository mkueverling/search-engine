{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7369e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from datasets import load_dataset\n",
    "# from tqdm import tqdm \n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569273fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations and data sources\n",
    "\n",
    "# # connect to to the local instance (run bin/elasticsearch in another terminal)\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# setting the index name\n",
    "index_name = \"trec_product_search\"\n",
    "\n",
    "# data source paths\n",
    "corpus_path = \"product_catalogue_esci.jsonl\"\n",
    "query_pathl = \"qid2query.tsv\"\n",
    "qrels_path = \"product-search-dev.qrels.txt\"\n",
    "\n",
    "# downloading nltk wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# obligatory connection test\n",
    "if es.ping():\n",
    "    info = es.info()\n",
    "    print(\"elasticsearch connected!\")\n",
    "else:\n",
    "    print(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a9eea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_settings = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"trec_analyzer\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\"lowercase\", \"my_stop_filter\", \"english_stemmer\"]\n",
    "        }\n",
    "      },\n",
    "      \"filter\": {\n",
    "        \"english_stemmer\": {\"type\": \"stemmer\", \"language\": \"english\"},\n",
    "        \"my_stop_filter\": {\"type\": \"stop\", \"stopwords\": \"_english_\"}\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"id\": {\"type\": \"keyword\"},\n",
    "      \"title\": {\"type\": \"text\", \"analyzer\": \"trec_analyzer\"},\n",
    "      \"contents\": {\"type\": \"text\", \"analyzer\": \"trec_analyzer\"},\n",
    "      \"brand\": {\"type\": \"text\", \"analyzer\": \"trec_analyzer\"}\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08442b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete index in case we break something\n",
    "if es.indices.exists(index=index_name):\n",
    "   es.indices.delete(index=index_name)\n",
    "\n",
    "# recreate index\n",
    "es.indices.create(index=index_name, body=index_settings)\n",
    "print(f\"index '{index_name}' created \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac53066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first look into the corpus\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "   for i in range(20):\n",
    "      print(json.loads(f.readline()))\n",
    "      \n",
    "\n",
    "# we find the fields 'product_id', 'product_title',  'product_description', 'product_bullet_point', 'product_brand', 'product_color_name', 'product_locale', 'trecid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_docs():\n",
    "  print(f\"Reading {corpus_path}...\")\n",
    "  with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "      try:\n",
    "        doc = json.loads(line)\n",
    "\n",
    "        # 1. ID MAPPING\n",
    "        # Prioritize 'trecid' for the assignment grading\n",
    "        doc_id = str(doc.get(\"trecid\") or doc.get(\"product_id\"))\n",
    "\n",
    "        # 2. KEY FIELDS\n",
    "        brand = doc.get(\"product_brand\", \"\") or \"\"\n",
    "        title = doc.get(\"product_title\", \"\") or \"\"\n",
    "\n",
    "        # 3. CONTENT MERGING (The \"All-In\" Strategy)\n",
    "        # Combine Description + Bullets + Color into one searchable blob\n",
    "        desc = doc.get(\"product_description\", \"\") or \"\"\n",
    "        bullets = doc.get(\"product_bullet_point\", \"\") or \"\"\n",
    "        color = doc.get(\"product_color_name\", \"\") or \"\"\n",
    "        \n",
    "        full_contents = f\"{desc} {bullets} {color}\"\n",
    "\n",
    "        yield {\n",
    "          \"_index\": index_name,\n",
    "          \"_id\": doc_id,\n",
    "          \"_source\": {\n",
    "            \"id\": doc_id,\n",
    "            \"title\": title,\n",
    "            \"contents\": full_contents,\n",
    "            \"brand\": brand,\n",
    "            # Metadata (not searched, but stored)\n",
    "            \"asin\": doc.get(\"product_id\"), \n",
    "            \"locale\": doc.get(\"product_locale\") \n",
    "          }\n",
    "        }\n",
    "      except json.JSONDecodeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac475c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BULK INDEXING LETS GO\n",
    "# !!!! change chunk_size if it doesn't perform well !!!!\n",
    "# took me 8m 15s with chunk_size = 5000\n",
    "success, failed = helpers.bulk(\n",
    "    es, \n",
    "    generate_docs(),\n",
    "    stats_only =True,\n",
    "    chunk_size=5000)\n",
    "\n",
    "print(f\"\\n done! success: {success}, failed: {failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727acf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask for english stop words\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def get_synonyms(word):\n",
    "    # filter out stop words into an empty list\n",
    "    if word.lower() in english_stopwords:\n",
    "        return []\n",
    "    \n",
    "    # it is recommended to filter out short words as they carry no helpful meaning\n",
    "    if len(word) < 3:\n",
    "        return []\n",
    "    \n",
    "    # again we use set so we avoid duplicates\n",
    "    found_synonyms = set()\n",
    "\n",
    "    # we use two loops with wordnet. one retrieves the synset (group), so the meaning,  and the second retieves the synonyms. e.g. bank has multiple meanings, like financial institution or river bank. the lemma loop retrives the synonyms in each of those meanings.\n",
    "    for group in wordnet.synsets(word):\n",
    "        for lemma in group.lemmas():\n",
    "            text = lemma.name()\n",
    "\n",
    "            # wordnet uses underscores so we replace that with a space\n",
    "            clean_text = text.replace('_', ' ')\n",
    "\n",
    "            # skip the words used in query\n",
    "            if clean_text.lower() != word.lower():\n",
    "                found_synonyms.add(clean_text)\n",
    "\n",
    "    return list(found_synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea42219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(user_query):\n",
    "    # split the query into tokens\n",
    "    tokens = user_query.split()\n",
    "    \n",
    "    # Step 2: Create a bucket to hold our new, massive list of words\n",
    "    final_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Always keep the original word!\n",
    "        final_tokens.append(token)\n",
    "        \n",
    "        # Get the synonyms for this specific word\n",
    "        synonyms = get_synonyms(token)\n",
    "        \n",
    "        # Add the synonyms to our bucket\n",
    "        # (We use 'extend' to add a list to a list)\n",
    "        final_tokens.extend(synonyms)\n",
    "        \n",
    "    # Step 3: Glue all the words back together into one giant string\n",
    "    return \" \".join(final_tokens)\n",
    "\n",
    "# --- 4. TEST IT ---\n",
    "print(\"\\n--- SYNONYM TEST ---\")\n",
    "test_phrase = \"mobile phone\"\n",
    "print(f\"Original: {test_phrase}\")\n",
    "print(f\"Expanded: {expand_query(test_phrase)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. DEFINE SEARCH WITH SYNONYMS (PLAN B)\n",
    "def search_products(user_query, top_k=10):\n",
    "    \n",
    "    # 1. Expand the query\n",
    "    # \"samsung case\" -> \"samsung case casing shell\"\n",
    "    expanded_query = expand_query(user_query)\n",
    "    \n",
    "    # 2. Construct the Search\n",
    "    query_body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": expanded_query, \n",
    "                \n",
    "                # We search in title and contents.\n",
    "                # Title is boosted (^3) so exact matches appear higher\n",
    "                \"fields\": [\"title^3\", \"contents\", \"brand^2\"],\n",
    "                \n",
    "                # CRITICAL: Use \"or\". If we used \"and\", the document would need \n",
    "                # to contain ALL synonyms (impossible).\n",
    "                \"operator\": \"or\" \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es.search(index=index_name, body=query_body)\n",
    "    return response['hits']['hits']\n",
    "\n",
    "# --- FINAL TEST ---\n",
    "print(\"--- SEARCH TEST ---\")\n",
    "results = search_products(\"mobile phone\")\n",
    "\n",
    "for i, hit in enumerate(results):\n",
    "    print(f\"{i+1}. {hit['_source']['title'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc588c3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7495f4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading WordNet...\n",
      "1. Loading Answer Key (QRELS)...\n",
      "   Found 8954 queries with answers.\n",
      "2. Starting Search Loop (showing progress every 100 queries)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marvin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/marvin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/marvin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/tmp/ipykernel_19453/787916976.py:136: DeprecationWarning: Received 'size' via a specific parameter in the presence of a 'body' parameter, which is deprecated and will be removed in a future version. Instead, use only 'body' or only specific parameters.\n",
      "  hits = es.search(index=index_name, size=100, body={\n",
      "/tmp/ipykernel_19453/787916976.py:144: DeprecationWarning: Received 'size' via a specific parameter in the presence of a 'body' parameter, which is deprecated and will be removed in a future version. Instead, use only 'body' or only specific parameters.\n",
      "  hits_opt = es.search(index=index_name, size=100, body={\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processed 100 queries... (63.5 q/sec)\n",
      "   Processed 200 queries... (68.5 q/sec)\n",
      "   Processed 300 queries... (70.7 q/sec)\n",
      "   Processed 400 queries... (73.0 q/sec)\n",
      "   Processed 500 queries... (74.3 q/sec)\n",
      "   Processed 600 queries... (75.3 q/sec)\n",
      "   Processed 700 queries... (75.8 q/sec)\n",
      "   Processed 800 queries... (76.0 q/sec)\n",
      "   Processed 900 queries... (76.0 q/sec)\n",
      "   Processed 1000 queries... (76.3 q/sec)\n",
      "   Processed 1100 queries... (76.6 q/sec)\n",
      "   Processed 1200 queries... (76.9 q/sec)\n",
      "   Processed 1300 queries... (77.2 q/sec)\n",
      "   Processed 1400 queries... (77.4 q/sec)\n",
      "   Processed 1500 queries... (77.2 q/sec)\n",
      "   Processed 1600 queries... (77.4 q/sec)\n",
      "   Processed 1700 queries... (77.6 q/sec)\n",
      "   Processed 1800 queries... (67.4 q/sec)\n",
      "   Processed 1900 queries... (68.0 q/sec)\n",
      "   Processed 2000 queries... (68.6 q/sec)\n",
      "   Processed 2100 queries... (69.2 q/sec)\n",
      "   Processed 2200 queries... (69.6 q/sec)\n",
      "   Processed 2300 queries... (69.5 q/sec)\n",
      "   Processed 2400 queries... (70.0 q/sec)\n",
      "   Processed 2500 queries... (70.3 q/sec)\n",
      "   Processed 2600 queries... (70.7 q/sec)\n",
      "   Processed 2700 queries... (71.1 q/sec)\n",
      "   Processed 2800 queries... (71.5 q/sec)\n",
      "   Processed 2900 queries... (71.8 q/sec)\n",
      "   Processed 3000 queries... (72.1 q/sec)\n",
      "   Processed 3100 queries... (72.3 q/sec)\n",
      "   Processed 3200 queries... (72.4 q/sec)\n",
      "   Processed 3300 queries... (72.7 q/sec)\n",
      "   Processed 3400 queries... (72.9 q/sec)\n",
      "   Processed 3500 queries... (73.1 q/sec)\n",
      "   Processed 3600 queries... (73.2 q/sec)\n",
      "   Processed 3700 queries... (73.3 q/sec)\n",
      "   Processed 3800 queries... (73.4 q/sec)\n",
      "   Processed 3900 queries... (73.5 q/sec)\n",
      "   Processed 4000 queries... (73.6 q/sec)\n",
      "   Processed 4100 queries... (73.6 q/sec)\n",
      "   Processed 4200 queries... (73.7 q/sec)\n",
      "   Processed 4300 queries... (73.9 q/sec)\n",
      "   Processed 4400 queries... (69.6 q/sec)\n",
      "   Processed 4500 queries... (69.8 q/sec)\n",
      "   Processed 4600 queries... (70.0 q/sec)\n",
      "   Processed 4700 queries... (70.1 q/sec)\n",
      "   Processed 4800 queries... (70.3 q/sec)\n",
      "   Processed 4900 queries... (70.5 q/sec)\n",
      "   Processed 5000 queries... (70.6 q/sec)\n",
      "   Processed 5100 queries... (70.7 q/sec)\n",
      "   Processed 5200 queries... (70.9 q/sec)\n",
      "   Processed 5300 queries... (71.0 q/sec)\n",
      "   Processed 5400 queries... (71.1 q/sec)\n",
      "   Processed 5500 queries... (71.3 q/sec)\n",
      "   Processed 5600 queries... (71.4 q/sec)\n",
      "   Processed 5700 queries... (71.5 q/sec)\n",
      "   Processed 5800 queries... (71.6 q/sec)\n",
      "   Processed 5900 queries... (71.7 q/sec)\n",
      "   Processed 6000 queries... (71.8 q/sec)\n",
      "   Processed 6100 queries... (71.9 q/sec)\n",
      "   Processed 6200 queries... (72.0 q/sec)\n",
      "   Processed 6300 queries... (72.1 q/sec)\n",
      "   Processed 6400 queries... (72.2 q/sec)\n",
      "   Processed 6500 queries... (72.3 q/sec)\n",
      "   Processed 6600 queries... (72.4 q/sec)\n",
      "   Processed 6700 queries... (72.5 q/sec)\n",
      "   Processed 6800 queries... (72.6 q/sec)\n",
      "   Processed 6900 queries... (70.0 q/sec)\n",
      "   Processed 7000 queries... (70.1 q/sec)\n",
      "   Processed 7100 queries... (70.2 q/sec)\n",
      "   Processed 7200 queries... (70.3 q/sec)\n",
      "   Processed 7300 queries... (70.4 q/sec)\n",
      "   Processed 7400 queries... (70.5 q/sec)\n",
      "   Processed 7500 queries... (70.6 q/sec)\n",
      "   Processed 7600 queries... (70.7 q/sec)\n",
      "   Processed 7700 queries... (70.8 q/sec)\n",
      "   Processed 7800 queries... (70.9 q/sec)\n",
      "   Processed 7900 queries... (71.0 q/sec)\n",
      "   Processed 8000 queries... (71.1 q/sec)\n",
      "   Processed 8100 queries... (71.2 q/sec)\n",
      "   Processed 8200 queries... (71.3 q/sec)\n",
      "   Processed 8300 queries... (71.4 q/sec)\n",
      "   Processed 8400 queries... (71.5 q/sec)\n",
      "   Processed 8500 queries... (71.5 q/sec)\n",
      "   Processed 8600 queries... (71.6 q/sec)\n",
      "   Processed 8700 queries... (71.7 q/sec)\n",
      "   Processed 8800 queries... (71.8 q/sec)\n",
      "   Processed 8900 queries... (71.9 q/sec)\n",
      "\n",
      "3. Calculating Grades...\n",
      "\n",
      "--- RESULTS: Baseline Run ---\n",
      "Recall@100: 0.0000\n",
      "MRR:        0.0000\n",
      "NDCG@100:   0.0000\n",
      "\n",
      "--- RESULTS: Optimized Run ---\n",
      "Recall@100: 0.0000\n",
      "MRR:        0.0000\n",
      "NDCG@100:   0.0000\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "\n",
    "# --- 1. SETUP RESOURCES ---\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    print(\"Downloading WordNet...\")\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "# --- 2. FAST SYNONYM LOGIC ---\n",
    "# We use a cache so we don't look up the same word (like \"phone\") 10,000 times\n",
    "synonym_cache = {}\n",
    "\n",
    "def get_synonyms(word):\n",
    "    if word in synonym_cache:\n",
    "        return synonym_cache[word]\n",
    "    \n",
    "    if word.lower() in english_stops or len(word) < 3:\n",
    "        synonym_cache[word] = []\n",
    "        return []\n",
    "    \n",
    "    found = set()\n",
    "    # OPTIMIZATION: Nouns only, top 2 meanings\n",
    "    try:\n",
    "        for group in wordnet.synsets(word, pos=wordnet.NOUN)[:2]:\n",
    "            for lemma in group.lemmas():\n",
    "                clean = lemma.name().replace('_', ' ')\n",
    "                if clean.lower() != word.lower(): found.add(clean)\n",
    "    except:\n",
    "        pass # Safety for weird characters\n",
    "        \n",
    "    result = list(found)\n",
    "    synonym_cache[word] = result\n",
    "    return result\n",
    "\n",
    "def expand_query(text):\n",
    "    tokens = text.split()\n",
    "    final = []\n",
    "    for t in tokens:\n",
    "        final.append(t)\n",
    "        final.extend(get_synonyms(t))\n",
    "    return \" \".join(list(set(final)))\n",
    "\n",
    "# --- 3. EVALUATOR ---\n",
    "class TrecEvaluator:\n",
    "    def __init__(self, qrels_path):\n",
    "        self.qrels = collections.defaultdict(set)\n",
    "        # Load QRELS first so we know which queries matter\n",
    "        with open(qrels_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 4 and int(parts[3]) > 0:\n",
    "                    self.qrels[parts[0]].add(parts[2])\n",
    "\n",
    "    def score(self, run_results, name):\n",
    "        recall, mrr, ndcg, count = 0, 0, 0, 0\n",
    "        for qid, rel_docs in self.qrels.items():\n",
    "            if qid not in run_results: continue\n",
    "            found = run_results[qid]\n",
    "            count += 1\n",
    "            \n",
    "            # Recall\n",
    "            hits = len(set(found) & rel_docs)\n",
    "            recall += hits / len(rel_docs)\n",
    "            \n",
    "            # MRR\n",
    "            for i, d in enumerate(found):\n",
    "                if d in rel_docs:\n",
    "                    mrr += 1.0 / (i + 1); break\n",
    "            \n",
    "            # NDCG\n",
    "            dcg, idcg = 0.0, 0.0\n",
    "            for i, d in enumerate(found):\n",
    "                if d in rel_docs: dcg += 1.0 / math.log2(i + 2)\n",
    "            for i in range(min(len(found), len(rel_docs))):\n",
    "                idcg += 1.0 / math.log2(i + 2)\n",
    "            if idcg > 0: ndcg += dcg / idcg\n",
    "\n",
    "        print(f\"\\n--- RESULTS: {name} ---\")\n",
    "        if count == 0:\n",
    "            print(\"WARNING: No matching queries found. Check IDs!\")\n",
    "        else:\n",
    "            print(f\"Recall@100: {recall/count:.4f}\")\n",
    "            print(f\"MRR:        {mrr/count:.4f}\")\n",
    "            print(f\"NDCG@100:   {ndcg/count:.4f}\")\n",
    "\n",
    "# --- 4. EXECUTION WITH PROGRESS BAR ---\n",
    "def run_project_fast():\n",
    "    print(\"1. Loading Answer Key (QRELS)...\")\n",
    "    # Load QRELS first to filter queries\n",
    "    valid_qids = set()\n",
    "    with open(\"product-search-dev.qrels.txt\", 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 1:\n",
    "                valid_qids.add(parts[0])\n",
    "    \n",
    "    print(f\"   Found {len(valid_qids)} queries with answers.\")\n",
    "    print(\"2. Starting Search Loop (showing progress every 100 queries)...\")\n",
    "    \n",
    "    f1 = open(\"run_baseline.txt\", \"w\")\n",
    "    f2 = open(\"run_optimized.txt\", \"w\")\n",
    "    \n",
    "    results_base = {}\n",
    "    results_opt = {}\n",
    "    \n",
    "    processed_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with open(\"qid2query.tsv\", 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 2: continue\n",
    "            qid, qtext = parts[0], parts[1]\n",
    "            \n",
    "            # OPTIMIZATION: Skip queries that won't be graded\n",
    "            if qid not in valid_qids:\n",
    "                continue\n",
    "            \n",
    "            processed_count += 1\n",
    "            if processed_count % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = processed_count / elapsed\n",
    "                print(f\"   Processed {processed_count} queries... ({rate:.1f} q/sec)\")\n",
    "            \n",
    "            # === A. BASELINE ===\n",
    "            hits = es.search(index=index_name, size=100, body={\n",
    "                \"query\": {\"multi_match\": {\"query\": qtext, \"fields\": [\"title\", \"contents\", \"brand\"]}}\n",
    "            })['hits']['hits']\n",
    "            results_base[qid] = [h['_id'] for h in hits]\n",
    "            for i, h in enumerate(hits):\n",
    "                f1.write(f\"{qid} Q0 {h['_id']} {i+1} {h['_score']:.4f} baseline\\n\")\n",
    "                \n",
    "            # === B. OPTIMIZED ===\n",
    "            hits_opt = es.search(index=index_name, size=100, body={\n",
    "                \"query\": {\"multi_match\": {\n",
    "                    \"query\": expand_query(qtext),\n",
    "                    \"fields\": [\"title^3\", \"contents\", \"brand^2\"],\n",
    "                    \"operator\": \"or\"\n",
    "                }}\n",
    "            })['hits']['hits']\n",
    "            results_opt[qid] = [h['_id'] for h in hits_opt]\n",
    "            for i, h in enumerate(hits_opt):\n",
    "                f2.write(f\"{qid} Q0 {h['_id']} {i+1} {h['_score']:.4f} optimized\\n\")\n",
    "                \n",
    "    f1.close(); f2.close()\n",
    "    \n",
    "    print(\"\\n3. Calculating Grades...\")\n",
    "    evaluator = TrecEvaluator(\"product-search-dev.qrels.txt\")\n",
    "    evaluator.score(results_base, \"Baseline Run\")\n",
    "    evaluator.score(results_opt, \"Optimized Run\")\n",
    "\n",
    "run_project_fast()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
