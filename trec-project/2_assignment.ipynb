{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7369e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569273fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "index_name = \"trec_product_search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b60f411a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing index: trec_product_search\n"
     ]
    }
   ],
   "source": [
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"Deleted existing index: {index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac53066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created index 'trec_product_search' with Metadata Augmentation strategy.\n"
     ]
    }
   ],
   "source": [
    "index_name = \"trec_product_search\"\n",
    "\n",
    "# Delete index if exists to ensure a clean slate\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    print(f\"‚ôªÔ∏è  Deleted old index: {index_name}\")\n",
    "\n",
    "settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0,\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                # LAB BOOK REQUIREMENT: HTML Stripping + English Processing\n",
    "                \"html_english_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"char_filter\": [\"html_strip\"],  # 1. Strips <br>, <div>, etc.\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",                # 2. Case insensitive\n",
    "                        \"english_stop\",             # 3. Removes stopwords\n",
    "                        \"english_stemmer\"           # 4. Stemming (run -> running)\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\"type\": \"stop\", \"stopwords\": \"_english_\"},\n",
    "                \"english_stemmer\": {\"type\": \"stemmer\", \"language\": \"english\"}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"docid\": {\"type\": \"keyword\"},\n",
    "            \"title\": {\"type\": \"text\", \"analyzer\": \"html_english_analyzer\"},\n",
    "            \"brand\": {\"type\": \"keyword\"}, # Keyword for Faceting/Filtering\n",
    "            \n",
    "            # --- PLAN B STRATEGY: METADATA AUGMENTATION ---\n",
    "            # We combine Title + Brand + Bullets here for maximum recall.\n",
    "            \"search_content\": {\n",
    "                \"type\": \"text\", \n",
    "                \"analyzer\": \"html_english_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "es.indices.create(index=index_name, body=settings)\n",
    "print(f\"‚úÖ Created index '{index_name}' with Metadata Augmentation strategy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "102b5813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåä Connecting to Hugging Face Data Stream...\n",
      "üöÄ Starting indexing... (This takes a few minutes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1118658it [04:28, 4162.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Indexing Complete!\n",
      "Indexed: 1118658 documents\n",
      "Failed: 0 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3. STREAM & INDEX CORPUS (The \"Direct Link\" Fix)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nüåä Connecting to Hugging Face Data Stream...\")\n",
    "\n",
    "# 1. We define the direct link to the raw data file\n",
    "# This bypasses the \"Dataset scripts are no longer supported\" error\n",
    "corpus_url = \"https://huggingface.co/datasets/trec-product-search/product-search-corpus/resolve/main/corpus.jsonl.gz\"\n",
    "\n",
    "# 2. We load it as a generic \"json\" file instead of a custom dataset\n",
    "# split=\"train\" ensures we get the data generator\n",
    "ds = load_dataset(\"json\", data_files=corpus_url, split=\"train\", streaming=True)\n",
    "\n",
    "def generate_actions():\n",
    "    # Iterate through the stream\n",
    "    for row in ds:\n",
    "        # Note: The raw JSON keys might slightly differ from the python script version\n",
    "        # We use .get() to be safe.\n",
    "        docid = row.get(\"docid\")\n",
    "        title = row.get(\"title\") or \"\"\n",
    "        brand = row.get(\"brand\") or \"\"\n",
    "        \n",
    "        # Handle bullet points (sometimes list, sometimes None)\n",
    "        bullets = row.get(\"bullet_points\")\n",
    "        if isinstance(bullets, list):\n",
    "            bullets = \" \".join(bullets)\n",
    "        elif bullets is None:\n",
    "            bullets = \"\"\n",
    "\n",
    "        # LAB BOOK LOGIC: Concatenate fields\n",
    "        combined_text = f\"{title} {brand} {bullets}\"\n",
    "\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_id\": docid,\n",
    "            \"_source\": {\n",
    "                \"docid\": docid,\n",
    "                \"title\": title,\n",
    "                \"brand\": brand,\n",
    "                \"search_content\": combined_text  # <-- Plan B Field\n",
    "            }\n",
    "        }\n",
    "\n",
    "print(\"üöÄ Starting indexing... (This takes a few minutes)\")\n",
    "\n",
    "# We use standard tqdm with a manual update\n",
    "successes, failed = helpers.bulk(es, tqdm(generate_actions(), mininterval=1.0), stats_only=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Indexing Complete!\")\n",
    "print(f\"Indexed: {successes} documents\")\n",
    "print(f\"Failed: {failed} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3854f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading Queries for verification...\n",
      "‚úÖ Loaded 116 queries.\n",
      "   299                       peplum top\n",
      "0  314  women light weight bikini pants\n",
      "1  312   drawstring shorts women casual\n",
      "2  170            avaivy facial product\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüì• Loading Queries for verification...\")\n",
    "# Using the resolved raw link\n",
    "query_url = \"https://huggingface.co/datasets/trec-product-search/product-search-2024-queries/resolve/main/2024_test_queries.tsv\"\n",
    "\n",
    "queries_df = pd.read_csv(query_url, sep=\"\\t\")\n",
    "print(f\"‚úÖ Loaded {len(queries_df)} queries.\")\n",
    "print(queries_df.head(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "es_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
