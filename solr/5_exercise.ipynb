{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be5cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metadata already exists.\n",
      "Loading documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing CSV: 192509it [00:02, 77702.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 191175 docs.\n",
      "‚úÖ Loaded 50 topics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "SOLR_URL = \"http://localhost:8983/solr\"\n",
    "INDEX_NAME = \"trec-covid-index\"\n",
    "LOCAL_TAR_PATH = \"data/cord-19_2020-07-16.tar.gz\"  # Your local file\n",
    "METADATA_PATH = \"data/2020-07-16/metadata.csv\"\n",
    "TOPICS_PATH = \"data/topics-rnd5.xml\"\n",
    "DOCIDS_PATH = \"data/docids-rnd5.txt\"\n",
    "\n",
    "# 1. Extract Metadata from Local File\n",
    "if not os.path.exists(METADATA_PATH):\n",
    "    if os.path.exists(LOCAL_TAR_PATH):\n",
    "        print(f\"Extracting metadata from {LOCAL_TAR_PATH}...\")\n",
    "        with tarfile.open(LOCAL_TAR_PATH, \"r:gz\") as tar:\n",
    "            tar.extract(tar.getmember(\"2020-07-16/metadata.csv\"), path=\"data\")\n",
    "        print(\"‚úÖ Metadata extracted.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: Could not find {LOCAL_TAR_PATH}. Please ensure it is in the 'data' folder.\")\n",
    "else:\n",
    "    print(\"‚úÖ Metadata already exists.\")\n",
    "\n",
    "# 2. Download Helper Files (Topics & DocIDs)\n",
    "files = {\n",
    "    DOCIDS_PATH: \"https://ir.nist.gov/trec-covid/data/docids-rnd5.txt\",\n",
    "    TOPICS_PATH: \"https://ir.nist.gov/trec-covid/data/topics-rnd5.xml\"\n",
    "}\n",
    "for path, url in files.items():\n",
    "    if not os.path.exists(path):\n",
    "        with open(path, 'wb') as f:\n",
    "            f.write(requests.get(url).content)\n",
    "\n",
    "# 3. Load Valid DocIDs (Set for speed)\n",
    "with open(DOCIDS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    valid_docids = set(line.strip() for line in f)\n",
    "\n",
    "# 4. Load Documents into Memory\n",
    "trec_covid_documents = []\n",
    "seen_ids = set()\n",
    "print(\"Loading documents...\")\n",
    "with open(METADATA_PATH, \"r\", encoding=\"utf-8\") as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    columns = next(reader)\n",
    "    for doc in tqdm(reader, desc=\"Parsing CSV\"):\n",
    "        d = dict(zip(columns, doc))\n",
    "        docid = d.get(\"cord_uid\")\n",
    "        if docid in valid_docids and docid not in seen_ids:\n",
    "            seen_ids.add(docid)\n",
    "            trec_covid_documents.append({\n",
    "                \"cord_uid\": docid,\n",
    "                \"title\": d.get(\"title\", \"\"),\n",
    "                \"abstract\": d.get(\"abstract\", \"\"),\n",
    "                # We only need year for boosting\n",
    "                \"publication_year\": d.get(\"publish_time\", \"\")[:4] \n",
    "            })\n",
    "print(f\"‚úÖ Loaded {len(trec_covid_documents)} docs.\")\n",
    "\n",
    "# 5. Load & Expand Topics (Query + Question)\n",
    "trec_topics = []\n",
    "tree = ET.parse(TOPICS_PATH)\n",
    "for topic in tree.getroot().findall(\"topic\"):\n",
    "    query = topic.find(\"query\").text\n",
    "    question = topic.find(\"question\").text\n",
    "    # Combine for better recall\n",
    "    full_query = f\"{query} {question}\" \n",
    "    trec_topics.append({\"id\": topic.get(\"number\"), \"query\": full_query})\n",
    "print(f\"‚úÖ Loaded {len(trec_topics)} topics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16cbabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Wiping Solr Core...\n",
      "Applying Schema...\n",
      "‚úÖ Schema Defined (Porter Stemmer).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 1. Hard Reset (Wipe Core)\n",
    "print(\"üßπ Wiping Solr Core...\")\n",
    "requests.get(f\"{SOLR_URL}/admin/cores?action=UNLOAD&core={INDEX_NAME}&deleteIndex=true&deleteInstanceDir=true\")\n",
    "requests.get(f\"{SOLR_URL}/admin/cores?action=CREATE&name={INDEX_NAME}&instanceDir={INDEX_NAME}&configSet=_default\")\n",
    "\n",
    "# 2. Define Schema with Porter Stemmer\n",
    "schema_payload = {\n",
    "    \"add-field-type\": {\n",
    "        \"name\": \"text_tuned\",\n",
    "        \"class\": \"solr.TextField\",\n",
    "        \"analyzer\": {\n",
    "            \"tokenizer\": { \"class\": \"solr.StandardTokenizerFactory\" },\n",
    "            \"filters\": [\n",
    "                { \"class\": \"solr.LowerCaseFilterFactory\" },\n",
    "                { \"class\": \"solr.StopFilterFactory\", \"words\": \"stopwords.txt\", \"ignoreCase\": \"true\" },\n",
    "                { \"class\": \"solr.EnglishPossessiveFilterFactory\" },\n",
    "                { \"class\": \"solr.PorterStemFilterFactory\" } # Aggressive Stemmer\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"add-field\": [\n",
    "        {\"name\": \"cord_uid\", \"type\": \"string\", \"stored\": True, \"indexed\": True},\n",
    "        {\"name\": \"title\", \"type\": \"text_tuned\", \"stored\": True, \"indexed\": True},\n",
    "        {\"name\": \"abstract\", \"type\": \"text_tuned\", \"stored\": True, \"indexed\": True},\n",
    "        {\"name\": \"publication_year\", \"type\": \"pint\", \"stored\": True, \"indexed\": True}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Applying Schema...\")\n",
    "requests.post(f\"{SOLR_URL}/{INDEX_NAME}/schema\", json=schema_payload)\n",
    "print(\"‚úÖ Schema Defined (Porter Stemmer).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf686283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Indexing 191175 documents...\n",
      "‚úÖ Indexing Complete.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f\"üöÄ Indexing {len(trec_covid_documents)} documents...\")\n",
    "resp = requests.post(\n",
    "    f\"{SOLR_URL}/{INDEX_NAME}/update\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    json=trec_covid_documents,\n",
    "    params={\"commit\": \"true\"}\n",
    ")\n",
    "if resp.status_code == 200:\n",
    "    print(\"‚úÖ Indexing Complete.\")\n",
    "else:\n",
    "    print(f\"‚ùå Indexing Failed: {resp.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea0544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ML Feature Extraction (This takes a minute)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:01<00:00, 23.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "ü§ñ ML RECOMMENDED WEIGHTS\n",
      "========================================\n",
      "Title Boost:    1.0\n",
      "Abstract Boost: 1.39\n",
      "Phrase Boost:   1.00\n",
      "========================================\n",
      "(I have pre-filled the next cell with robust defaults, but you can update them with these numbers!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load Ground Truth\n",
    "qrels = requests.get(\"https://ir.nist.gov/trec-covid/data/qrels-covid_d5_j0.5-5.txt\").text\n",
    "qrel_dict = {}\n",
    "for line in qrels.strip().split('\\n'):\n",
    "    qid, _, docid, rel = line.split()\n",
    "    qrel_dict[(qid, docid)] = int(rel)\n",
    "\n",
    "# 2. Extract Features\n",
    "print(\"Running ML Feature Extraction (This takes a minute)...\")\n",
    "X = [] # Features\n",
    "y = [] # Labels\n",
    "\n",
    "# We use a subset of topics to train quickly\n",
    "for topic in tqdm(trec_topics[:25], desc=\"Training\"):\n",
    "    qid = topic['id']\n",
    "    # Remove stopwords for cleaner feature matching\n",
    "    q_clean = \" \".join([w for w in topic['query'].split() if w.lower() not in {\"what\",\"is\",\"the\",\"of\",\"in\"}])\n",
    "    \n",
    "    # Get scores for Title, Abstract, and Phrase separately\n",
    "    try:\n",
    "        s_title = requests.get(f\"{SOLR_URL}/{INDEX_NAME}/select\", params={\"q\": q_clean, \"defType\": \"edismax\", \"qf\": \"title\", \"fl\": \"cord_uid,score\", \"rows\": 50}).json()\n",
    "        s_abst = requests.get(f\"{SOLR_URL}/{INDEX_NAME}/select\", params={\"q\": q_clean, \"defType\": \"edismax\", \"qf\": \"abstract\", \"fl\": \"cord_uid,score\", \"rows\": 50}).json()\n",
    "        s_phrase = requests.get(f\"{SOLR_URL}/{INDEX_NAME}/select\", params={\"q\": q_clean, \"defType\": \"edismax\", \"qf\": \"title\", \"pf\": \"title\", \"ps\": \"5\", \"fl\": \"cord_uid,score\", \"rows\": 50}).json()\n",
    "        \n",
    "        scores_t = {d['cord_uid']: d['score'] for d in s_title.get('response',{}).get('docs',[])}\n",
    "        scores_a = {d['cord_uid']: d['score'] for d in s_abst.get('response',{}).get('docs',[])}\n",
    "        scores_p = {d['cord_uid']: d['score'] for d in s_phrase.get('response',{}).get('docs',[])}\n",
    "        \n",
    "        all_docs = set(scores_t.keys()) | set(scores_a.keys()) | set(scores_p.keys())\n",
    "        \n",
    "        for doc in all_docs:\n",
    "            if (qid, doc) in qrel_dict:\n",
    "                X.append([scores_t.get(doc,0), scores_a.get(doc,0), scores_p.get(doc,0)])\n",
    "                y.append(qrel_dict[(qid, doc)])\n",
    "    except: pass\n",
    "\n",
    "# 3. Train & Print\n",
    "if X:\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    w = model.coef_\n",
    "    base = w[0] if w[0] > 0.001 else 1.0\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"ü§ñ ML RECOMMENDED WEIGHTS\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Title Boost:    1.0\")\n",
    "    print(f\"Abstract Boost: {abs(w[1]/base):.2f}\")\n",
    "    print(f\"Phrase Boost:   {abs(w[2]/base):.2f}\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"(I have pre-filled the next cell with robust defaults, but you can update them with these numbers!)\")\n",
    "else:\n",
    "    print(\"‚ùå ML failed to find training data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d91969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running HIGH RECALL Search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:02<00:00, 24.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Run file created: run_high_recall.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "RUN_FILENAME = \"run_high_recall.txt\"\n",
    "RUN_NAME = \"high_recall_run\"\n",
    "\n",
    "# STOPWORDS - Keep these to reduce noise slightly, but we will be looser elsewhere\n",
    "STOPWORDS = {\"what\", \"is\", \"the\", \"of\", \"in\", \"on\", \"to\", \"and\", \"a\", \"an\", \"for\", \"with\", \"are\", \"do\", \"does\", \"how\"}\n",
    "\n",
    "print(f\"Running HIGH RECALL Search...\")\n",
    "with open(RUN_FILENAME, 'w') as f:\n",
    "    for topic in tqdm(trec_topics, desc=\"Searching\"):\n",
    "        topic_id = topic['id']\n",
    "        \n",
    "        # 1. Clean Query\n",
    "        clean_terms = [word for word in topic['query'].split() if word.lower() not in STOPWORDS]\n",
    "        clean_query = \" \".join(clean_terms)\n",
    "\n",
    "        # 2. High Recall Params\n",
    "        params = {\n",
    "            \"q\": clean_query,\n",
    "            \"defType\": \"edismax\",\n",
    "            \n",
    "            # WIDE NET: Search Title AND Abstract\n",
    "            # Reduced boosting gap to allow abstract matches to surface\n",
    "            \"qf\": \"title^2 abstract^1\", \n",
    "            \n",
    "            # PHRASE BOOST: Keep this high to ensure top results are good (Precision)\n",
    "            \"pf\": \"title^10 abstract^5\",\n",
    "            \"ps\": 15,       # Very loose phrase slop (words can be far apart)\n",
    "            \n",
    "            # NO 'mm' parameter! (Allow any match)\n",
    "            # This maximizes the number of docs we retrieve\n",
    "            \n",
    "            \"rows\": 1000,\n",
    "            \n",
    "            # RERANKING: Keep the recent paper boost, it's a solid heuristic\n",
    "            \"rq\": f'{{!rerank reRankQuery=\"publication_year:2020^10 OR publication_year:2021^10\" reRankDocs=1000 reRankWeight=2.0}}',\n",
    "            \n",
    "            \"fl\": \"cord_uid,score\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            resp = requests.get(f\"{SOLR_URL}/{INDEX_NAME}/select\", params=params)\n",
    "            docs = resp.json().get('response', {}).get('docs', [])\n",
    "            for rank, doc in enumerate(docs):\n",
    "                f.write(f\"{topic_id} Q0 {doc['cord_uid']} {rank+1} {doc['score']} {RUN_NAME}\\n\")\n",
    "        except: pass\n",
    "\n",
    "print(f\"‚úÖ Run file created: {RUN_FILENAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bd6d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üèÜ FINAL RESULTS\n",
      "========================================\n",
      "MAP:        0.2222\n",
      "Prec@100:   0.5470\n",
      "Recall@100: 0.1221\n",
      "========================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pytrec_eval\n",
    "\n",
    "# Load QRELs\n",
    "qrels = requests.get(\"https://ir.nist.gov/trec-covid/data/qrels-covid_d5_j0.5-5.txt\").text\n",
    "qrel = {}\n",
    "for line in qrels.strip().split('\\n'):\n",
    "    qid, _, docid, rel = line.split()\n",
    "    if qid not in qrel: qrel[qid] = {}\n",
    "    qrel[qid][docid] = int(rel)\n",
    "\n",
    "# Load Run\n",
    "run = {}\n",
    "with open(RUN_FILENAME, 'r') as f:\n",
    "    for line in f:\n",
    "        qid, _, docid, _, score, _ = line.split()\n",
    "        if qid not in run: run[qid] = {}\n",
    "        run[qid][docid] = float(score)\n",
    "\n",
    "# Score\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrel, {'map', 'P_100', 'recall_100'})\n",
    "res = evaluator.evaluate(run)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"MAP:        {sum(x['map'] for x in res.values()) / len(res):.4f}\")\n",
    "print(f\"Prec@100:   {sum(x['P_100'] for x in res.values()) / len(res):.4f}\")\n",
    "print(f\"Recall@100: {sum(x['recall_100'] for x in res.values()) / len(res):.4f}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90e3d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Parameters used\n",
    "\n",
    "# \"defType\": \"edismax\",\n",
    "\n",
    "# \"qf\": \"title^2 abstract^1\", \n",
    "\n",
    "# \"pf\": \"title^10 abstract^5\",\n",
    "# \"ps\": 15,     \n",
    "\n",
    "# \"rows\": 1000,\n",
    "\n",
    "# \"rq\": f'{{!rerank reRankQuery=\"publication_year:2020^10 OR publication_year:2021^10\" reRankDocs=1000 reRankWeight=2.0}}', # push relevancy for recent papers\n",
    "\n",
    "# \"fl\": \"cord_uid,score\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
